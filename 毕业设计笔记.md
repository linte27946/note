# 笔记

## 深度学习基础知识

### 感知机

<img src="/home/kobayashi/桌面/毕业设计笔记/picture/微信图片_2025-02-27_141714_994.png" style="zoom: 33%;" /> 

#### 前馈神经网络（Feedforward Neural Network，FNN）

前馈神经网络（Feedforward Neural Network，FNN）是神经网络家族中的基本单元。

前馈神经网络特点是数据从输入层开始，经过一个或多个隐藏层，最后到达输出层，全过程没有循环或反馈。

**前馈神经网络的基本结构：**

- **输入层：** 数据进入网络的入口点。输入层的每个节点代表一个输入特征。
- **隐藏层：**一个或多个层，用于捕获数据的非线性特征。每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力。
- **输出层：**输出网络的预测结果。节点数和问题类型相关，例如分类问题的输出节点数等于类别数。
- **连接权重与偏置：**每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递。
  ![](/home/kobayashi/桌面/毕业设计笔记/picture/neural-net.png) 

### 损失函数：

**目的：用于衡量模型预测与实际目标值之间误差的函数。它是模型训练的核心，帮助优化算法（如梯度下降）决定如何更新模型参数，以最小化预测错误。**

**举例：计算线性回归**

用拟合函数来模拟数据点连成的曲线，输入为x，输出为y 我们需要确定参数，不同的参数会得到不同的曲线。损失函数用来衡量拟合结果与实际结果的偏差程度，是一个打分机器。

![截图 2025-02-27 14-12-49](/home/kobayashi/桌面/毕业设计笔记/picture/截图 2025-02-27 14-12-49.png)

**训练过程：调节参数，降低损失函数**

### 梯度下降：

**目标是通过不断调整模型参数，使得损失函数的值逐步减小。**

### 神经网络

前向传播、损失计算、反向传播和参数更新。

### 反向传播：



## Pytorch使用

### 基础

张量：可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。

- **创建n*n\*n......的张量**

  ```py
  #张量
  t1 = torch.tensor([1.0, 2.0, 3.0])
  #随机张量
  x=torch.randn(3,3)
  print(x)
  #零张量
  y=torch.zeros(3,3)
  print(y)
  #全1张量
  z=torch.ones(3,3)
  print(z)
  ```

  ***

  **`torch.tensor`**

  - 作用：从 Python 的列表、元组或 NumPy 数组创建一个张量。
  - 语法：`torch.tensor(data, dtype=None, device=None, requires_grad=False)`
  - `data`：可以是列表、元组、NumPy 数组等。
  - `dtype`：指定数据类型，如 `torch.float32`、`torch.int64` 等（默认根据输入数据推测）。
  - `device`：指定运行设备（CPU/GPU）。
  - `requires_grad`：是否需要计算梯度（用于自动求导）。

  ***

- **运算**

  ```py
  a = torch.randn(2, 2)
  b = torch.randn(2, 2)
  
  # 张量加法
  c = a + b
  print(c)
  
  # 张量乘法
  d = a * b
  print(d)
  
  # 矩阵乘法
  e = torch.matmul(a, b)
  print(e)
  ```

  ***

  - **张量加法**：合并不同的特征或层输出，例如残差连接和多层网络的输出合并。
  - 张量乘法：
    - **逐元素乘法**：加权计算、特征掩码、数据增强等。
    - **矩阵乘法**：全连接层的加权求和、卷积操作、图神经网络传播等。
  - **张量转置**：调整数据维度、优化计算图、确保矩阵乘法的维度匹配。
  - **广播机制**：简化代码，自动调整维度，使不同形状的张量能够进行算术运算。

  ***

- **选择GPU**

  ```py
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  
  x=torch.randn(3,3).to(device)
  ```

- **梯度和自动微分**

  PyTorch的张量支持自动微分，这是深度学习中的关键特性。当你创建一个需要梯度的张量时，PyTorch可以自动计算其梯度：backward()和grad()方法

  ```py
  x = torch.randn(3, 3, requires_grad=True)  # 需要梯度计算
  
  y = x * 2  # 进行一些运算
  y.backward()  # 反向传播，计算梯度
  print(x.grad)  # 打印 x 的梯度
  ```

  ### 神经网络

  PyTorch 提供了许多常见的神经网络层，以下是几个常见的：

  - **`nn.Linear(in_features, out_features)`**：全连接层，输入 `in_features` 个特征，输出 `out_features` 个特征。
  - **`nn.Conv2d(in_channels, out_channels, kernel_size)`**：2D 卷积层，用于图像处理。
  - **`nn.MaxPool2d(kernel_size)`**：2D 最大池化层，用于降维。
  - **`nn.ReLU()`**：ReLU 激活函数，常用于隐藏层。
  - **`nn.Softmax(dim)`**：Softmax 激活函数，通常用于输出层，适用于多类分类问题。

- **激活函数（Activation Function）**

  激活函数决定了神经元是否应该被激活。它们是非线性函数，使得神经网络能够学习和执行更复杂的任务。常见的激活函数包括：

  - Sigmoid：用于二分类问题，输出值在 0 和 1 之间。
  - Tanh：输出值在 -1 和 1 之间，常用于输出层之前。
  - ReLU（Rectified Linear Unit）：目前最流行的激活函数之一，定义为 `f(x) = max(0, x)`，有助于解决梯度消失问题。
  - Softmax：常用于多分类问题的输出层，将输出转换为概率分布。

  ```py
  import torch.nn.functional as F
  
  # ReLU 激活
  output = F.relu(input_tensor)
  
  # Sigmoid 激活
  output = torch.sigmoid(input_tensor)
  
  # Tanh 激活
  output = torch.tanh(input_tensor)
  ```

- **损失函数**

  常见的损失函数包括：

  - **均方误差（MSELoss）**：回归问题常用，计算输出与目标值的平方差。
  - **交叉熵损失（CrossEntropyLoss）**：分类问题常用，计算输出和真实标签之间的交叉熵。
  - **BCEWithLogitsLoss**：二分类问题，结合了 Sigmoid 激活和二元交叉熵损失。

  ```py
  # 均方误差损失
  criterion = nn.MSELoss()
  
  # 交叉熵损失
  criterion = nn.CrossEntropyLoss()
  
  # 二分类交叉熵损失
  criterion = nn.BCEWithLogitsLoss()
  ```

- **优化器（Optimizer）**

  优化器负责在训练过程中更新网络的权重和偏置。

  常见的优化器包括：

  - SGD（随机梯度下降）

  - Adam（自适应矩估计）

  - RMSprop（均方根传播）

  ```py
  import torch.optim as optim
  
  # 使用 SGD 优化器
  optimizer = optim.SGD(model.parameters(), lr=0.01)
  
  # 使用 Adam 优化器
  optimizer = optim.Adam(model.parameters(), lr=0.001)
  ```

- **构建神经网络**

  神经网络通过调整神经元之间的连接权重来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。

  PyTorch 提供了一个非常方便的接口来构建神经网络模型，即 torch.nn.Module。

  我们可以继承 nn.Module 类并定义自己的网络层。

  ```py
  import torch.nn as nn
  import torch
  
  # 定义一个简单的全连接神经网络
  class SimpleNN(nn.Module):
      def __init__(self):
          super(SimpleNN, self).__init__()
          self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层
          self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层
      
      def forward(self, x):
          x = torch.relu(self.fc1(x))  # ReLU 激活函数
          x = self.fc2(x)
          return x
  
  # 创建网络实例
  model = SimpleNN()
  
  # 打印模型结构
  print(model)
  ```

  ***

  super作用：用来调用父类的方法，例子中是调用`nn.Model`的构造函数来进行初始化。

  对比直接使用父类：如果在多重继承中调用 `nn.Model.__init__()`，可能会导致**同一个父类的 `__init__` 被执行多次**，引发错误。

  `super()` 采用 **MRO（方法解析顺序）**，可以正确解析继承关系，避免重复调用。

  在单继承和多继承中，都推荐使用 `super()`，而不是直接调用父类的方法。

  在 Python 3 及以上版本，我们可以省略 `self`，写成：`super().__init__()`

  ***

- **训练过程：**

  1. **前向传播（Forward Propagation）**：    在前向传播阶段，输入数据通过网络层传递，每层应用权重和激活函数，直到产生输出。
  2. **计算损失（Calculate Loss）**：    根据网络的输出和真实标签，计算损失函数的值。
  3. **反向传播（Backpropagation）**：    反向传播利用自动求导技术计算损失函数关于每个参数的梯度。
  4. **参数更新（Parameter Update）**：    使用优化器根据梯度更新网络的权重和偏置。
  5. **迭代（Iteration）**：    重复上述过程，直到模型在训练数据上的性能达到满意的水平。

```py
import torch
import torch.nn as nn
import torch.optim as optim

# 1. 定义一个简单的神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层
        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))  # ReLU 激活函数
        x = self.fc2(x)
        return x

# 2. 创建模型实例
model = SimpleNN()

# 3. 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差损失函数
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器

# 4. 假设我们有训练数据 X 和 Y
X = torch.randn(10, 2)  # 10 个样本，2 个特征
Y = torch.randn(10, 1)  # 10 个目标值

# 5. 训练循环
for epoch in range(100):  # 训练 100 轮
    optimizer.zero_grad()  # 清空之前的梯度
    output = model(X)  # 前向传播
    loss = criterion(output, Y)  # 计算损失
    loss.backward()  # 反向传播
    optimizer.step()  # 更新参数
    
    # 每 10 轮输出一次损失
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

## 代码阅读

### `def get_parser()`: 定义了命令行的参数。

#### 参数优先级: 命令行 > 配置文件 > 默认值

- **`--work-dir`**
   用于存储结果的工作目录，默认是 `./work_dir/temp`。
- **`-model_saved_name`**
   存储的模型名称，默认为空。
- **`--config`**
   配置文件的路径，默认是 `./config/nturgbd-cross-view/test_bone.yaml`。

#### processor 处理

- **`--phase`**
   训练阶段，必须为 `train` 或 `test`，默认是 `train`。
- **`--save-score`**
   是否保存分类得分，布尔类型，默认是 `False`。

#### visulize and debug 可视化和调试

- **`--seed`**
   PyTorch 的随机种子，默认值为 `1`。
- **`--log-interval`**
   训练时打印日志的间隔（以迭代次数计），默认值为 `100`。
- **`--save-interval`**
   训练时保存模型的间隔（以迭代次数计），默认值为 `1`。
- **`--save-epoch`**
   从哪个 epoch 开始保存模型，默认值为 `30`。
- **`--eval-interval`**
   评估模型的间隔（以迭代次数计），默认值为 `5`。
- **`--print-log`**
   是否打印日志，布尔类型，默认值为 `True`。
- **`--show-topk`**
   显示 Top-K 精度，默认是 `[1, 5]`，可以传入多个值。

####  feeder

- **`--feeder`**
   数据加载器的路径，默认是 `'feeder.feeder'`。
- **`--num-worker`**
   数据加载器的 worker 数量，默认值为 `32`。
- **`--train-feeder-args`**
   训练数据加载器的参数，默认为空字典 `{}`。
- **`--test-feeder-args`**
   测试数据加载器的参数，默认为空字典 `{}`。

#### model 模型

- **`--model`**
   要使用的模型，默认值为 `None`。
- **`--model-args`**
   传递给模型的参数，默认为空字典 `{}`。
- **`--weights`**
   用于模型初始化的权重文件，默认值为 `None`。
- **`--ignore-weights`**
   初始化时要忽略的权重名称列表，默认是空列表 `[]`，可以传入多个值。

#### optim 优化

- **`--base-lr`**
   初始学习率，默认值为 `0.01`。
- **`--step`**
   优化器降低学习率的 epoch 位置，默认是 `[20, 40, 60]`，可以传入多个值。
- **`--device`**
   训练或测试时使用的 GPU 设备编号，默认是 `0`，可以传入多个值。
- **`--optimizer`**
   优化器类型，默认是 `'SGD'`。
- **`--nesterov`**
   是否使用 Nesterov 动量，布尔类型，默认值为 `False`。
- **`--batch-size`**
   训练时的 batch 大小，默认值为 `256`。
- **`--test-batch-size`**
   测试时的 batch 大小，默认值为 `256`。
- **`--start-epoch`**
   训练从哪个 epoch 开始，默认值为 `0`。
- **`--num-epoch`**
   训练总 epoch 数，默认值为 `80`。
- **`--weight-decay`**
   优化器的权重衰减系数，默认值为 `0.0005`。
- **`--lr-ratio`**
   学习率衰减比例，默认值为 `0.001`。
- **`--lr-decay-rate`**
   学习率衰减率，默认值为 `0.1`。
- **`--warm_up_epoch`**
   预热训练的 epoch 数，默认值为 `0`。
- **`--loss-type`**
   损失函数类型，默认值为 `'CE'`（交叉熵）。

### init_seed(seed)

**因为在深度学习任务中，随机性会影响模型的训练所以设定随机种子，使实验结果可复现**

### processor类

#### `__init__`

```py
if phase=='train'
	表示模型处于训练阶段，需要设置训练日志目录
	if `debug` 模式关闭
		创建 model_saved_name 目录路径，用于存储训练过程的日志、模型权重等文件。
		if 如果日志目录已存在
			则提示用户是否删除旧的日志。防止日志目录混乱，确保每次训练都是一个干净的环境。
			if 用户选择 'y'
            	则删除旧目录，提醒用户刷新 TensorBoard。
			else 
            	保留旧的训练日志。
			创建 TensorBoard 记录器：
    		训练日志 (train_writer) 存储在 runs/train
    		验证日志 (val_writer) 存储在 runs/val
    else
    	表示debug 模式开启，日志只存放到test目录，避免浪费存储空间。
记录训练步数 (global_step)，用于控制学习率调整、日志记录等。
调用 load_model() 方法，加载模型结构，并尝试载入预训练权重（如果有）。

if phase =='model_size'
	说明只想计算模型大小，不需要加载优化器和数据。
else
    self.load_optimizer()：加载优化器（如 Adam、SGD）。
    self.load_data()：加载数据集（训练集、测试集）。

设置初始学习率 (base_lr 可能是 arg 里定义的超参数)。
记录最佳精度 (best_acc) 及其对应的 epoch (best_acc_epoch)。
将模型 (self.model) 移动到指定的 GPU (self.output_device)。

if arg.device 是列表
	if 包含多个 GPU：
        使用 nn.DataParallel 进行多 GPU 训练。
        device_ids=self.arg.device 指定要使用的 GPU。
        output_device=self.output_device 设定主 GPU（用于 loss 计算）。
```

#### load_model()

✔ **确定计算设备** ✔ **动态导入模型** ✔ **实例化模型** ✔ **设置损失函数** ✔ **加载预训练权重** ✔ **删除不需要的权重** ✔ **兼容不同权重加载方式**

#### load_data()

### HDGCN.py(核心)

这段代码实现了一个结合多尺度时间卷积（TCN）和层次分解图卷积（HD-GCN）的深度学习模型，用于处理骨骼关键点数据的动作识别任务（如NTU RGB+D数据集）。1. 核心组件解析

1. **时间卷积模块**
- `TemporalConv`：基础时间卷积单元，使用2D卷积处理时间维度
- `MultiScale_TemporalConv`：多分支结构，包含：
  - 不同膨胀率的时序卷积分支
  - 最大池化分支
  - 1x1卷积分支
  - 残差连接设计

2. **图卷积模块**
- `EdgeConv`：动态边卷积
  - 通过k-NN构建局部图结构
  - 使用特征差（feature - x）和原始特征拼接
  - 最大池化聚合邻居信息
- `HD_Gconv`：层次分解图卷积
  - 使用可学习的自适应邻接矩阵（PA参数）
  - 结合多个子图卷积和边卷积
  - 支持注意力机制（AHA模块）

3. **注意力机制**
- `AHA`（Attention Hierarchy Aggregation）：
  - 通过分层采样关节特征
  - 使用边卷积计算层级间注意力
  - 加权聚合不同层次特征

4. **基本单元**
- `TCN_GCN_unit`：将图卷积与时序卷积结合的基础模块
  - 顺序执行：GCN → TCN → 残差连接
  - 通道数变化和下采样功能

### 2. 模型架构（Model类）

1. **输入处理**
- 输入形状：(N, C, T, V, M) → (Batch, 通道, 时间帧, 关节点, 人数)
- 数据归一化：通过BatchNorm1D处理

2. **网络结构**
10层TCN-GCN单元，分三个阶段：
- 阶段1（l1-l4）：64通道，保持分辨率
- 阶段2（l5-l7）：128通道，时间维度下采样
- 阶段3（l8-l10）：256通道，再次下采样

3. **输出层**
- 时空全局平均池化
- 全连接层分类

### 3. 关键设计特点

1. **多尺度时序建模**
- 使用不同膨胀率的卷积核（dilations=[1,2]）
- 结合最大池化和1x1卷积分支
- 增强时间维度的特征提取能力

2. **自适应图结构**
- 可学习的邻接矩阵（PA参数）
- 动态边卷积构建局部图关系
- 支持层次化的关节分组（通过get_groups）

3. **注意力机制**
- 层级注意力（AHA）聚合不同层次特征
- 基于关节运动模式（CoM）的分组策略

4. **残差设计**
- 各模块均包含残差连接
- 通过downsample匹配维度
- 使用ReLU保持非线性

### 4. 数据流示例（前向传播）
输入： (N, 3, 300, 25, 2) → NTU数据集典型输入
1. 预处理：
   - 重排为 (N*M, C, T, V)
   - BatchNorm标准化
2. 通过10层TCN-GCN：
   - 通道变化：3 → 64 → 128 → 256
   - 时间维度逐步下采样（stride=2）
3. 输出处理：
   - 时空池化 → (N, 256)
   - Dropout → 全连接 → 分类得分

### 5. 创新点总结
1. 层次分解图卷积：结合预定义解剖结构和自适应学习
2. 多尺度时空建模：同时捕捉局部和全局时空特征
3. 注意力引导的特征聚合：增强关键关节和关键帧的表示

该模型通过融合图卷积的拓扑建模能力和时间卷积的序列建模优势，在骨骼动作识别任务中表现出较好的性能。代码实现中大量使用einops张量操作库，提升了可读性和维护性。

# 笔记

## 深度学习基础知识

### 感知机

<img src="/home/kobayashi/桌面/毕业设计笔记/picture/微信图片_2025-02-27_141714_994.png" style="zoom: 33%;" /> 

### 损失函数：

**目的：用于衡量模型预测与实际目标值之间误差的函数。它是模型训练的核心，帮助优化算法（如梯度下降）决定如何更新模型参数，以最小化预测错误。**

**举例：计算线性回归**

用拟合函数来模拟数据点连成的曲线，输入为x，输出为y 我们需要确定参数，不同的参数会得到不同的曲线。损失函数用来衡量拟合结果与实际结果的偏差程度，是一个打分机器。

![截图 2025-02-27 14-12-49](/home/kobayashi/桌面/毕业设计笔记/picture/截图 2025-02-27 14-12-49.png)

**训练过程：调节参数，降低损失函数**

### 梯度下降：

**目标是通过不断调整模型参数，使得损失函数的值逐步减小。**

### 神经网络

前向传播、损失计算、反向传播和参数更新。

### 反向传播：



## Pytorch使用

张量：可以看作是一个多维数组。标量可以看作是0维张量，向量可以看作1维张量，矩阵可以看作是二维张量。

- **创建n*n\*n......的张量**

  ```py
  #张量
  t1 = torch.tensor([1.0, 2.0, 3.0])
  #随机张量
  x=torch.randn(3,3)
  print(x)
  #零张量
  y=torch.zeros(3,3)
  print(y)
  #全1张量
  z=torch.ones(3,3)
  print(z)
  ```

  ***

  **`torch.tensor`**

  - 作用：从 Python 的列表、元组或 NumPy 数组创建一个张量。
  - 语法：`torch.tensor(data, dtype=None, device=None, requires_grad=False)`
  - `data`：可以是列表、元组、NumPy 数组等。
  - `dtype`：指定数据类型，如 `torch.float32`、`torch.int64` 等（默认根据输入数据推测）。
  - `device`：指定运行设备（CPU/GPU）。
  - `requires_grad`：是否需要计算梯度（用于自动求导）。

  ***

- **运算**

  ```py
  a = torch.randn(2, 2)
  b = torch.randn(2, 2)
  
  # 张量加法
  c = a + b
  print(c)
  
  # 张量乘法
  d = a * b
  print(d)
  
  # 矩阵乘法
  e = torch.matmul(a, b)
  print(e)
  ```

  ***

  - **张量加法**：合并不同的特征或层输出，例如残差连接和多层网络的输出合并。
  - 张量乘法：
    - **逐元素乘法**：加权计算、特征掩码、数据增强等。
    - **矩阵乘法**：全连接层的加权求和、卷积操作、图神经网络传播等。
  - **张量转置**：调整数据维度、优化计算图、确保矩阵乘法的维度匹配。
  - **广播机制**：简化代码，自动调整维度，使不同形状的张量能够进行算术运算。

  ***

- **选择GPU**

  ```py
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  
  x=torch.randn(3,3).to(device)
  ```

- **梯度和自动微分**

  PyTorch的张量支持自动微分，这是深度学习中的关键特性。当你创建一个需要梯度的张量时，PyTorch可以自动计算其梯度：backward()和grad()方法

  ```py
  x = torch.randn(3, 3, requires_grad=True)  # 需要梯度计算
  
  y = x * 2  # 进行一些运算
  y.backward()  # 反向传播，计算梯度
  print(x.grad)  # 打印 x 的梯度
  ```

- **构建神经网络**

  神经网络通过调整神经元之间的连接权重来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新。

  PyTorch 提供了一个非常方便的接口来构建神经网络模型，即 torch.nn.Module。

  我们可以继承 nn.Module 类并定义自己的网络层。

  ```py
  import torch.nn as nn
  import torch
  
  # 定义一个简单的全连接神经网络
  class SimpleNN(nn.Module):
      def __init__(self):
          super(SimpleNN, self).__init__()
          self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层
          self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层
      
      def forward(self, x):
          x = torch.relu(self.fc1(x))  # ReLU 激活函数
          x = self.fc2(x)
          return x
  
  # 创建网络实例
  model = SimpleNN()
  
  # 打印模型结构
  print(model)
  ```

  ***

  super作用：用来调用父类的方法，例子中是调用`nn.Model`的构造函数来进行初始化。

  对比直接使用父类：如果在多重继承中调用 `nn.Model.__init__()`，可能会导致**同一个父类的 `__init__` 被执行多次**，引发错误。

  `super()` 采用 **MRO（方法解析顺序）**，可以正确解析继承关系，避免重复调用。

  在单继承和多继承中，都推荐使用 `super()`，而不是直接调用父类的方法。

  在 Python 3 及以上版本，我们可以省略 `self`，写成：`super().__init__()`

  ***

- **训练过程：**

  1. **前向传播（Forward Propagation）**：    在前向传播阶段，输入数据通过网络层传递，每层应用权重和激活函数，直到产生输出。
  2. **计算损失（Calculate Loss）**：    根据网络的输出和真实标签，计算损失函数的值。
  3. **反向传播（Backpropagation）**：    反向传播利用自动求导技术计算损失函数关于每个参数的梯度。
  4. **参数更新（Parameter Update）**：    使用优化器根据梯度更新网络的权重和偏置。
  5. **迭代（Iteration）**：    重复上述过程，直到模型在训练数据上的性能达到满意的水平。

```py
import torch
import torch.nn as nn
import torch.optim as optim

# 1. 定义一个简单的神经网络模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 2)  # 输入层到隐藏层
        self.fc2 = nn.Linear(2, 1)  # 隐藏层到输出层
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))  # ReLU 激活函数
        x = self.fc2(x)
        return x

# 2. 创建模型实例
model = SimpleNN()

# 3. 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差损失函数
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 优化器

# 4. 假设我们有训练数据 X 和 Y
X = torch.randn(10, 2)  # 10 个样本，2 个特征
Y = torch.randn(10, 1)  # 10 个目标值

# 5. 训练循环
for epoch in range(100):  # 训练 100 轮
    optimizer.zero_grad()  # 清空之前的梯度
    output = model(X)  # 前向传播
    loss = criterion(output, Y)  # 计算损失
    loss.backward()  # 反向传播
    optimizer.step()  # 更新参数
    
    # 每 10 轮输出一次损失
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
```

